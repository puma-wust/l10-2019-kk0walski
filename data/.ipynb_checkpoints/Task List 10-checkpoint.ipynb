{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018/2019 - Task List 10\n",
    "\n",
    "1. Implement Naive Bayes classifier with pyro\n",
    "    - create apropriate parameters (mean and std for a and b, sigma - noise)\n",
    "    - provide optimization procedure\n",
    "    - check appropriateness of implemented method with selected dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro.optim as optim\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, TracePredictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes(nn.Module):\n",
    "    \"\"\" Implementation of Naive Bayes as a layer for pytorch models\n",
    "    TODO\n",
    "    ----\n",
    "    - Make std devs fixable\n",
    "    - Look into better param initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, features, classes):\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.classes = classes\n",
    "        self.means = nn.Parameter(torch.zeros(classes, features))\n",
    "        self.variances = nn.Parameter(torch.ones(classes, features))\n",
    "        self.class_priors = nn.Parameter(torch.Tensor(self.classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        normal = Normal(self.means, torch.abs(self.variances))\n",
    "        x = x[:,np.newaxis,:]\n",
    "        reasult = torch.sum(normal.log_prob(x), dim=-1) + torch.log(self.class_priors)\n",
    "        return reasult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "feature_train_v = torch.FloatTensor(X_train)\n",
    "labels_train_v = torch.FloatTensor(y_train)\n",
    "feature_test_v = torch.FloatTensor(X_test)\n",
    "labels_test_v = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Cauchy, Categorical, Normal, Beta\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from tqdm import tqdm\n",
    "softmax = nn.Softmax(dim=1)\n",
    "my_class_priors = torch.Tensor(np.histogram(labels_train_v, bins=[i for i in range(4)])[0]/len(labels_train_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model = GaussianNaiveBayes(4, 3)\n",
    "bayes_model.class_priors.data = my_class_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    # weight and bias priors\n",
    "    mean_prior = Normal(loc=torch.zeros_like(bayes_model.means),\n",
    "                        scale=torch.ones_like(bayes_model.variances)).independent(2)\n",
    "    var_prior = Normal(loc=torch.zeros_like(bayes_model.means),\n",
    "                        scale=torch.ones_like(bayes_model.variances)).independent(2)\n",
    "        \n",
    "    priors = {'means': mean_prior, 'variances': var_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", bayes_model, priors)\n",
    "    # sample a nn (which also samples w and b)\n",
    "    lifted_bayes_model = lifted_module()\n",
    "    with pyro.plate(\"map\", len(x_data)):\n",
    "        # run the nn forward on data\n",
    "        prediction_mean = lifted_bayes_model(x_data)\n",
    "        # condition on the observed data\n",
    "        hat = softmax(prediction_mean)\n",
    "        obs = pyro.sample(\"obs\", Categorical(logits=hat), obs=y_data)\n",
    "        return hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(x_data, y_data):\n",
    "    mean_prior = Normal(loc=torch.rand_like(bayes_model.means)*5,\n",
    "                        scale=torch.ones_like(bayes_model.variances)).independent(2)\n",
    "    mean_prior_param = pyro.param('means', mean_prior)\n",
    "    var_prior = Normal(loc=torch.rand_like(bayes_model.means)*2,\n",
    "                        scale=torch.ones_like(bayes_model.variances)).independent(2)\n",
    "    var_prior_param = pyro.param('variances', var_prior)\n",
    "    \n",
    "    priors = {'means': mean_prior, 'variances': var_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", bayes_model, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pyro.clear_param_store()\n",
    "    num_iterations=100\n",
    "    optim = pyro.optim.Adam({\"lr\": 0.01})\n",
    "    svi = pyro.infer.SVI(model, guide, optim, loss=pyro.infer.Trace_ELBO(), num_samples=50)\n",
    "    t=tqdm(range(num_iterations))\n",
    "    for j in t:\n",
    "        loss = svi.step(feature_train_v,  labels_train_v)\n",
    "        t.set_postfix(loss=loss)\n",
    "    return (model, svi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
